import polars as pl
import numpy as np
from typing import List, Dict
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# Removed PDF/TXT loaders - JSON only processing
from langchain_community.embeddings import HuggingFaceEmbeddings
import json
import os
from pathlib import Path

class TRIZPatentLabeler:
    def __init__(self):
        """Initialize the TRIZ Patent Labeler with 48 engineering parameters.
        
        REFERENCES:
        - TRIZ Parameters: Altshuller, G. (1984). "Creativity as an Exact Science"
        - Ensemble Methods: Polikar, R. (2006). "Ensemble based systems in decision making"
        - Patent Analysis: Yoon, B. & Park, Y. (2004). "A text-mining-based patent network"
        - See REFERENCES.md for complete bibliography
        
        METHODOLOGY:
        Uses ensemble approach combining:
        1. Keyword matching (30% weight) - Domain-specific term matching
        2. Semantic similarity (40% weight) - Contextual understanding via embeddings  
        3. TF-IDF analysis (30% weight) - Statistical text analysis
        
        LIMITATIONS:
        - Keywords are hand-crafted, not corpus-derived
        - Fixed weights, not learned from training data
        - No validation against ground truth dataset
        """
        self.triz_parameters = {
            1: "Weight of moving object",
            2: "Weight of stationary object", 
            3: "Length of moving object",
            4: "Length of stationary object",
            5: "Area of moving object",
            6: "Area of stationary object",
            7: "Volume of moving object",
            8: "Volume of stationary object",
            9: "Speed",
            10: "Force",
            11: "Stress or pressure",
            12: "Shape",
            13: "Stability of the object's composition",
            14: "Strength",
            15: "Duration of action of moving object",
            16: "Duration of action of stationary object",
            17: "Temperature",
            18: "Illumination intensity",
            19: "Use of energy by moving object",
            20: "Use of energy by stationary object",
            21: "Power",
            22: "Loss of energy",
            23: "Loss of substance",
            24: "Loss of information",
            25: "Loss of time",
            26: "Quantity of substance/the matter",
            27: "Reliability",
            28: "Measurement accuracy",
            29: "Manufacturing precision",
            30: "External harm affects the object",
            31: "Object-generated harmful factors",
            32: "Ease of manufacture",
            33: "Ease of operation",
            34: "Ease of repair",
            35: "Adaptability or versatility",
            36: "Device complexity",
            37: "Difficulty of detecting and measuring",
            38: "Extent of automation",
            39: "Productivity",
            40: "Harmful factors acting on the object",
            41: "Harmful factors generated by the object",
            42: "Capacity or throughput",
            43: "Quality",
            44: "Convenience of use",
            45: "Level of automation",
            46: "Compatibility",
            47: "Safety",
            48: "Accuracy of measurement"
        }
        
        # Keywords associated with each parameter for better matching
        self.parameter_keywords = {
            1: ["weight", "mass", "heavy", "light", "moving", "mobile", "dynamic"],
            2: ["weight", "mass", "heavy", "light", "stationary", "fixed", "static"],
            3: ["length", "long", "short", "dimension", "moving", "mobile"],
            4: ["length", "long", "short", "dimension", "stationary", "fixed"],
            5: ["area", "surface", "coverage", "moving", "mobile"],
            6: ["area", "surface", "coverage", "stationary", "fixed"],
            7: ["volume", "capacity", "size", "moving", "mobile"],
            8: ["volume", "capacity", "size", "stationary", "fixed"],
            9: ["speed", "velocity", "fast", "slow", "rate", "acceleration"],
            10: ["force", "pressure", "push", "pull", "tension", "compression"],
            11: ["stress", "pressure", "strain", "load", "mechanical"],
            12: ["shape", "form", "geometry", "configuration", "design"],
            13: ["stability", "composition", "structure", "integrity", "consistent"],
            14: ["strength", "durability", "robust", "strong", "weak", "mechanical"],
            15: ["duration", "time", "period", "lifetime", "moving", "action"],
            16: ["duration", "time", "period", "lifetime", "stationary", "action"],
            17: ["temperature", "heat", "cold", "thermal", "heating", "cooling"],
            18: ["illumination", "light", "brightness", "lighting", "visibility"],
            19: ["energy", "power", "consumption", "efficiency", "moving", "dynamic"],
            20: ["energy", "power", "consumption", "efficiency", "stationary", "static"],
            21: ["power", "energy", "electrical", "mechanical", "output"],
            22: ["loss", "waste", "energy", "efficiency", "dissipation"],
            23: ["loss", "waste", "substance", "material", "leakage"],
            24: ["loss", "information", "data", "signal", "communication"],
            25: ["loss", "time", "delay", "efficiency", "speed"],
            26: ["quantity", "amount", "substance", "material", "mass"],
            27: ["reliability", "dependable", "consistent", "failure", "robust"],
            28: ["measurement", "accuracy", "precision", "error", "calibration"],
            29: ["manufacturing", "precision", "tolerance", "production", "quality"],
            30: ["external", "harm", "damage", "environment", "protection"],
            31: ["harmful", "factors", "generated", "byproduct", "side effect"],
            32: ["manufacture", "production", "fabrication", "easy", "difficult"],
            33: ["operation", "use", "user", "interface", "easy", "difficult"],
            34: ["repair", "maintenance", "service", "fix", "easy", "difficult"],
            35: ["adaptability", "versatility", "flexible", "adjustable", "modular"],
            36: ["complexity", "simple", "complicated", "device", "system"],
            37: ["detecting", "measuring", "sensing", "monitoring", "difficulty"],
            38: ["automation", "automatic", "manual", "control", "extent"],
            39: ["productivity", "output", "efficiency", "throughput", "performance"],
            40: ["harmful", "factors", "acting", "external", "damage"],
            41: ["harmful", "factors", "generated", "internal", "byproduct"],
            42: ["capacity", "throughput", "volume", "flow", "processing"],
            43: ["quality", "performance", "standard", "excellence", "grade"],
            44: ["convenience", "usability", "user", "comfort", "ergonomic"],
            45: ["automation", "automatic", "level", "degree", "control"],
            46: ["compatibility", "interoperability", "standard", "interface"],
            47: ["safety", "secure", "protection", "hazard", "risk"],
            48: ["accuracy", "precision", "measurement", "error", "calibration"]
        }
        
        # Initialize embeddings model
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        # Initialize TF-IDF vectorizer
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
    def load_patent_json(self, json_file_path: str) -> List[Dict]:
        """Load patent documents from JSON file."""
        with open(json_file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def combine_patent_text(self, patent_doc: Dict) -> str:
        """Combine different sections of a patent document into a single text."""
        sections = []
        
        # Add title if available
        if 'title' in patent_doc and patent_doc['title']:
            sections.append(f"Title: {patent_doc['title']}")
        
        # Add abstract if available
        if 'abstract' in patent_doc and patent_doc['abstract']:
            sections.append(f"Abstract: {patent_doc['abstract']}")
        
        # Add description if available
        if 'description' in patent_doc and patent_doc['description']:
            sections.append(f"Description: {patent_doc['description']}")
        
        # Add claims if available
        if 'claims' in patent_doc and patent_doc['claims']:
            sections.append(f"Claims: {patent_doc['claims']}")
        
        return ' '.join(sections)
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess patent text for analysis."""
        # Remove extra whitespace and newlines
        text = re.sub(r'\s+', ' ', text)
        # Convert to lowercase
        text = text.lower()
        # Remove special characters but keep periods and commas
        text = re.sub(r'[^\w\s.,]', '', text)
        return text.strip()
    
    def extract_patent_sections(self, text: str) -> Dict[str, str]:
        """Extract relevant sections from patent text."""
        sections = {
            'abstract': '',
            'claims': '',
            'description': ''
        }
        
        # Simple regex patterns to identify sections
        patterns = {
            'abstract': r'abstract[:\s]*(.*?)(?=\n\n|\nclaims?|background)',
            'claims': r'claims?[:\s]*(.*?)(?=\n\n|description|background)',
            'description': r'(?:detailed\s+)?description[:\s]*(.*?)(?=\n\n|claims?|abstract)'
        }
        
        for section, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections[section] = match.group(1).strip()
        
        # If no sections found, use entire text as description
        if not any(sections.values()):
            sections['description'] = text
            
        return sections
    
    def calculate_keyword_scores(self, text: str) -> Dict[int, float]:
        """Calculate scores for each TRIZ parameter based on keyword matching."""
        scores = {}
        text_lower = text.lower()
        
        for param_id, keywords in self.parameter_keywords.items():
            score = 0
            for keyword in keywords:
                # Count occurrences of keyword
                count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                score += count
            
            # Normalize by text length
            scores[param_id] = score / len(text.split()) if text.split() else 0
            
        return scores
    
    def calculate_semantic_similarity(self, text: str) -> Dict[int, float]:
        """Calculate semantic similarity between text and TRIZ parameters."""
        # Create embeddings for the text
        text_embedding = self.embeddings_model.embed_query(text)
        
        scores = {}
        for param_id, param_desc in self.triz_parameters.items():
            # Create embedding for parameter description + keywords
            param_text = param_desc + " " + " ".join(self.parameter_keywords[param_id])
            param_embedding = self.embeddings_model.embed_query(param_text)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(
                [text_embedding], 
                [param_embedding]
            )[0][0]
            scores[param_id] = similarity
            
        return scores
    
    def calculate_tfidf_scores(self, text: str, reference_corpus: List[str] = None) -> Dict[int, float]:
        """Calculate TF-IDF based scores for TRIZ parameters."""
        if reference_corpus is None:
            # Use parameter descriptions as reference corpus
            reference_corpus = [
                f"{desc} {' '.join(self.parameter_keywords[param_id])}"
                for param_id, desc in self.triz_parameters.items()
            ]
        
        # Add the input text to corpus
        corpus = reference_corpus + [text]
        
        # Fit TF-IDF vectorizer
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
        
        # Calculate similarity between text and each parameter
        text_vector = tfidf_matrix[-1]  # Last item is the input text
        param_vectors = tfidf_matrix[:-1]  # All others are parameter descriptions
        
        similarities = cosine_similarity(text_vector, param_vectors)[0]
        
        scores = {}
        for i, param_id in enumerate(self.triz_parameters.keys()):
            scores[param_id] = similarities[i]
            
        return scores
    
    def ensemble_scoring(self, text: str, weights: Dict[str, float] = None) -> Dict[int, float]:
        """Combine multiple scoring methods using ensemble approach."""
        if weights is None:
            weights = {
                'keyword': 0.2,
                'semantic': 0.5,
                'tfidf': 0.3
            }
        
        # Calculate scores using different methods
        keyword_scores = self.calculate_keyword_scores(text)
        semantic_scores = self.calculate_semantic_similarity(text)
        tfidf_scores = self.calculate_tfidf_scores(text)
        
        # Normalize scores to 0-1 range
        def normalize_scores(scores):
            max_score = max(scores.values()) if scores.values() else 1
            if max_score == 0.0:
                max_score = 1
            return {k: v / max_score for k, v in scores.items()}
        
        keyword_scores = normalize_scores(keyword_scores)
        semantic_scores = normalize_scores(semantic_scores)
        tfidf_scores = normalize_scores(tfidf_scores)
        
        # Combine scores
        final_scores = {}
        for param_id in self.triz_parameters.keys():
            final_scores[param_id] = (
                weights['keyword'] * keyword_scores.get(param_id, 0) +
                weights['semantic'] * semantic_scores.get(param_id, 0) +
                weights['tfidf'] * tfidf_scores.get(param_id, 0)
            )
        
        return final_scores
    
    def label_patent_from_json(self, patent_doc: Dict, top_k: int = 5, threshold: float = 0.1) -> Dict:
        """Label a patent document from JSON data with TRIZ parameters."""
        # Combine patent text from JSON fields
        text = self.combine_patent_text(patent_doc)
        processed_text = self.preprocess_text(text)
        
        # Extract sections from the combined text
        sections = self.extract_patent_sections(processed_text)
        
        # Calculate scores for each section
        section_scores = {}
        for section_name, section_text in sections.items():
            if section_text:
                section_scores[section_name] = self.ensemble_scoring(section_text)
        
        # Calculate overall scores (weighted average of sections)
        section_weights = {
            'abstract': 0.3,
            'claims': 0.4,
            'description': 0.3
        }
        
        overall_scores = {}
        for param_id in self.triz_parameters.keys():
            weighted_score = 0
            total_weight = 0
            
            for section_name, scores in section_scores.items():
                if section_name in section_weights:
                    weight = section_weights[section_name]
                    weighted_score += weight * scores.get(param_id, 0)
                    total_weight += weight
            
            overall_scores[param_id] = weighted_score / total_weight if total_weight > 0 else 0
        
        # Filter and sort results
        filtered_scores = {
            param_id: score for param_id, score in overall_scores.items()
            if score >= threshold
        }
        
        # Get top-k parameters
        top_parameters = sorted(
            filtered_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        # Prepare results
        results = {
            'patent_id': patent_doc.get('id', 'unknown'),
            'doc_num': patent_doc.get('doc_num', 'unknown'),
            'title': patent_doc.get('title', 'unknown'),
            'top_parameters': [
                {
                    'id': param_id,
                    'name': self.triz_parameters[param_id],
                    'score': score,
                    'confidence': 'high' if score > 0.7 else 'medium' if score > 0.4 else 'low'
                }
                for param_id, score in top_parameters
            ],
            'all_scores': overall_scores,
            'section_analysis': {
                section: {
                    'top_parameter': max(scores.items(), key=lambda x: x[1]) if scores else None,
                    'avg_score': np.mean(list(scores.values())) if scores else 0
                }
                for section, scores in section_scores.items()
            }
        }
        
        return results
    
    def batch_label_patents_from_json(self, json_file_path: str, output_file: str = None, limit: int = None) -> List[Dict]:
        """Label multiple patent documents from JSON file."""
        print(f"Loading patents from: {json_file_path}")
        patent_docs = self.load_patent_json(json_file_path)
        
        if limit:
            patent_docs = patent_docs[:limit]
            print(f"Processing first {limit} patents...")
        
        results = []
        total_patents = len(patent_docs)
        
        for i, patent_doc in enumerate(patent_docs, 1):
            try:
                print(f"Processing patent {i}/{total_patents}: {patent_doc.get('title', 'Unknown title')[:50]}...")
                result = self.label_patent_from_json(patent_doc)
                results.append(result)
            except Exception as e:
                print(f"Error processing patent {patent_doc.get('id', 'unknown')}: {e}")
                continue
        
        # Save results if output file specified
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"Results saved to: {output_file}")
        
        return results
    
    def export_to_csv(self, results: List[Dict], output_file: str):
        """Export results to CSV format using polars."""
        rows = []
        
        for result in results:
            # JSON-based result
            base_row = {
                'patent_id': result.get('patent_id', 'unknown'),
                'doc_num': result.get('doc_num', 'unknown'),
                'title': result.get('title', 'unknown')[:100] + '...' if len(result.get('title', '')) > 100 else result.get('title', 'unknown')
            }
            
            # Add top parameters
            for i, param in enumerate(result['top_parameters'][:5]):  # Top 5
                base_row[f'param_{i+1}_id'] = param['id']
                base_row[f'param_{i+1}_name'] = param['name']
                base_row[f'param_{i+1}_score'] = param['score']
                base_row[f'param_{i+1}_confidence'] = param['confidence']
            
            rows.append(base_row)
        
        # Create polars DataFrame and export to CSV
        df = pl.DataFrame(rows)
        df.write_csv(output_file)
        print(f"CSV exported to: {output_file}")


def main():
    """Main function to demonstrate usage."""
    labeler = TRIZPatentLabeler()
    
    print("TRIZ Patent Labeler - JSON Processing")
    print("=====================================")
    
    json_file_path = input("Enter JSON file path: ")
    if os.path.exists(json_file_path):
        limit_input = input("Enter limit (number of patents to process, or press Enter for all): ")
        limit = int(limit_input) if limit_input.strip() else None
        
        output_file = input("Enter output JSON file name (or press Enter for default): ") or "patent_triz_results.json"
        
        results = labeler.batch_label_patents_from_json(json_file_path, output_file, limit)
        print(f"\nProcessed {len(results)} patents")
        print(f"Results saved to: {output_file}")
        
        # Ask if user wants CSV export
        csv_export = input("\nExport to CSV? (y/n): ").lower() == 'y'
        if csv_export:
            csv_file = input("Enter CSV filename (or press Enter for default): ") or "patent_triz_results.csv"
            labeler.export_to_csv(results, csv_file)
    else:
        print("JSON file not found!")


if __name__ == "__main__":
    main()
