import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
import json
import os
from pathlib import Path

class TRIZPatentLabeler:
    def __init__(self):
        """Initialize the TRIZ Patent Labeler with 48 engineering parameters."""
        self.triz_parameters = {
            1: "Weight of moving object",
            2: "Weight of stationary object", 
            3: "Length of moving object",
            4: "Length of stationary object",
            5: "Area of moving object",
            6: "Area of stationary object",
            7: "Volume of moving object",
            8: "Volume of stationary object",
            9: "Speed",
            10: "Force",
            11: "Stress or pressure",
            12: "Shape",
            13: "Stability of the object's composition",
            14: "Strength",
            15: "Duration of action of moving object",
            16: "Duration of action of stationary object",
            17: "Temperature",
            18: "Illumination intensity",
            19: "Use of energy by moving object",
            20: "Use of energy by stationary object",
            21: "Power",
            22: "Loss of energy",
            23: "Loss of substance",
            24: "Loss of information",
            25: "Loss of time",
            26: "Quantity of substance/the matter",
            27: "Reliability",
            28: "Measurement accuracy",
            29: "Manufacturing precision",
            30: "External harm affects the object",
            31: "Object-generated harmful factors",
            32: "Ease of manufacture",
            33: "Ease of operation",
            34: "Ease of repair",
            35: "Adaptability or versatility",
            36: "Device complexity",
            37: "Difficulty of detecting and measuring",
            38: "Extent of automation",
            39: "Productivity",
            40: "Harmful factors acting on the object",
            41: "Harmful factors generated by the object",
            42: "Capacity or throughput",
            43: "Quality",
            44: "Convenience of use",
            45: "Level of automation",
            46: "Compatibility",
            47: "Safety",
            48: "Accuracy of measurement"
        }
        
        # Keywords associated with each parameter for better matching
        self.parameter_keywords = {
            1: ["weight", "mass", "heavy", "light", "moving", "mobile", "dynamic"],
            2: ["weight", "mass", "heavy", "light", "stationary", "fixed", "static"],
            3: ["length", "long", "short", "dimension", "moving", "mobile"],
            4: ["length", "long", "short", "dimension", "stationary", "fixed"],
            5: ["area", "surface", "coverage", "moving", "mobile"],
            6: ["area", "surface", "coverage", "stationary", "fixed"],
            7: ["volume", "capacity", "size", "moving", "mobile"],
            8: ["volume", "capacity", "size", "stationary", "fixed"],
            9: ["speed", "velocity", "fast", "slow", "rate", "acceleration"],
            10: ["force", "pressure", "push", "pull", "tension", "compression"],
            11: ["stress", "pressure", "strain", "load", "mechanical"],
            12: ["shape", "form", "geometry", "configuration", "design"],
            13: ["stability", "composition", "structure", "integrity", "consistent"],
            14: ["strength", "durability", "robust", "strong", "weak", "mechanical"],
            15: ["duration", "time", "period", "lifetime", "moving", "action"],
            16: ["duration", "time", "period", "lifetime", "stationary", "action"],
            17: ["temperature", "heat", "cold", "thermal", "heating", "cooling"],
            18: ["illumination", "light", "brightness", "lighting", "visibility"],
            19: ["energy", "power", "consumption", "efficiency", "moving", "dynamic"],
            20: ["energy", "power", "consumption", "efficiency", "stationary", "static"],
            21: ["power", "energy", "electrical", "mechanical", "output"],
            22: ["loss", "waste", "energy", "efficiency", "dissipation"],
            23: ["loss", "waste", "substance", "material", "leakage"],
            24: ["loss", "information", "data", "signal", "communication"],
            25: ["loss", "time", "delay", "efficiency", "speed"],
            26: ["quantity", "amount", "substance", "material", "mass"],
            27: ["reliability", "dependable", "consistent", "failure", "robust"],
            28: ["measurement", "accuracy", "precision", "error", "calibration"],
            29: ["manufacturing", "precision", "tolerance", "production", "quality"],
            30: ["external", "harm", "damage", "environment", "protection"],
            31: ["harmful", "factors", "generated", "byproduct", "side effect"],
            32: ["manufacture", "production", "fabrication", "easy", "difficult"],
            33: ["operation", "use", "user", "interface", "easy", "difficult"],
            34: ["repair", "maintenance", "service", "fix", "easy", "difficult"],
            35: ["adaptability", "versatility", "flexible", "adjustable", "modular"],
            36: ["complexity", "simple", "complicated", "device", "system"],
            37: ["detecting", "measuring", "sensing", "monitoring", "difficulty"],
            38: ["automation", "automatic", "manual", "control", "extent"],
            39: ["productivity", "output", "efficiency", "throughput", "performance"],
            40: ["harmful", "factors", "acting", "external", "damage"],
            41: ["harmful", "factors", "generated", "internal", "byproduct"],
            42: ["capacity", "throughput", "volume", "flow", "processing"],
            43: ["quality", "performance", "standard", "excellence", "grade"],
            44: ["convenience", "usability", "user", "comfort", "ergonomic"],
            45: ["automation", "automatic", "level", "degree", "control"],
            46: ["compatibility", "interoperability", "standard", "interface"],
            47: ["safety", "secure", "protection", "hazard", "risk"],
            48: ["accuracy", "precision", "measurement", "error", "calibration"]
        }
        
        # Initialize embeddings model
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        # Initialize TF-IDF vectorizer
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
    def load_patent_document(self, file_path: str) -> str:
        """Load patent document from various formats."""
        file_path = Path(file_path)
        
        if file_path.suffix.lower() == '.pdf':
            loader = PyPDFLoader(str(file_path))
            docs = loader.load()
            return ' '.join([doc.page_content for doc in docs])
        elif file_path.suffix.lower() == '.txt':
            loader = TextLoader(str(file_path))
            docs = loader.load()
            return docs[0].page_content
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess patent text for analysis."""
        # Remove extra whitespace and newlines
        text = re.sub(r'\s+', ' ', text)
        # Convert to lowercase
        text = text.lower()
        # Remove special characters but keep periods and commas
        text = re.sub(r'[^\w\s.,]', '', text)
        return text.strip()
    
    def extract_patent_sections(self, text: str) -> Dict[str, str]:
        """Extract relevant sections from patent text."""
        sections = {
            'abstract': '',
            'claims': '',
            'description': '',
            'summary': ''
        }
        
        # Simple regex patterns to identify sections
        patterns = {
            'abstract': r'abstract[:\s]*(.*?)(?=\n\n|\nclaims?|background|summary)',
            'claims': r'claims?[:\s]*(.*?)(?=\n\n|description|background)',
            'summary': r'summary[:\s]*(.*?)(?=\n\n|claims?|description)',
            'description': r'(?:detailed\s+)?description[:\s]*(.*?)(?=\n\n|claims?|abstract)'
        }
        
        for section, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections[section] = match.group(1).strip()
        
        # If no sections found, use entire text as description
        if not any(sections.values()):
            sections['description'] = text
            
        return sections
    
    def calculate_keyword_scores(self, text: str) -> Dict[int, float]:
        """Calculate scores for each TRIZ parameter based on keyword matching."""
        scores = {}
        text_lower = text.lower()
        
        for param_id, keywords in self.parameter_keywords.items():
            score = 0
            for keyword in keywords:
                # Count occurrences of keyword
                count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                score += count
            
            # Normalize by text length
            scores[param_id] = score / len(text.split()) if text.split() else 0
            
        return scores
    
    def calculate_semantic_similarity(self, text: str) -> Dict[int, float]:
        """Calculate semantic similarity between text and TRIZ parameters."""
        # Create embeddings for the text
        text_embedding = self.embeddings_model.embed_query(text)
        
        scores = {}
        for param_id, param_desc in self.triz_parameters.items():
            # Create embedding for parameter description + keywords
            param_text = param_desc + " " + " ".join(self.parameter_keywords[param_id])
            param_embedding = self.embeddings_model.embed_query(param_text)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(
                [text_embedding], 
                [param_embedding]
            )[0][0]
            scores[param_id] = similarity
            
        return scores
    
    def calculate_tfidf_scores(self, text: str, reference_corpus: List[str] = None) -> Dict[int, float]:
        """Calculate TF-IDF based scores for TRIZ parameters."""
        if reference_corpus is None:
            # Use parameter descriptions as reference corpus
            reference_corpus = [
                f"{desc} {' '.join(self.parameter_keywords[param_id])}"
                for param_id, desc in self.triz_parameters.items()
            ]
        
        # Add the input text to corpus
        corpus = reference_corpus + [text]
        
        # Fit TF-IDF vectorizer
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
        
        # Calculate similarity between text and each parameter
        text_vector = tfidf_matrix[-1]  # Last item is the input text
        param_vectors = tfidf_matrix[:-1]  # All others are parameter descriptions
        
        similarities = cosine_similarity(text_vector, param_vectors)[0]
        
        scores = {}
        for i, param_id in enumerate(self.triz_parameters.keys()):
            scores[param_id] = similarities[i]
            
        return scores
    
    def ensemble_scoring(self, text: str, weights: Dict[str, float] = None) -> Dict[int, float]:
        """Combine multiple scoring methods using ensemble approach."""
        if weights is None:
            weights = {
                'keyword': 0.3,
                'semantic': 0.4,
                'tfidf': 0.3
            }
        
        # Calculate scores using different methods
        keyword_scores = self.calculate_keyword_scores(text)
        semantic_scores = self.calculate_semantic_similarity(text)
        tfidf_scores = self.calculate_tfidf_scores(text)
        
        # Normalize scores to 0-1 range
        def normalize_scores(scores):
            max_score = max(scores.values()) if scores.values() else 1
            return {k: v / max_score for k, v in scores.items()}
        
        keyword_scores = normalize_scores(keyword_scores)
        semantic_scores = normalize_scores(semantic_scores)
        tfidf_scores = normalize_scores(tfidf_scores)
        
        # Combine scores
        final_scores = {}
        for param_id in self.triz_parameters.keys():
            final_scores[param_id] = (
                weights['keyword'] * keyword_scores.get(param_id, 0) +
                weights['semantic'] * semantic_scores.get(param_id, 0) +
                weights['tfidf'] * tfidf_scores.get(param_id, 0)
            )
        
        return final_scores
    
    def label_patent(self, file_path: str, top_k: int = 5, threshold: float = 0.1) -> Dict:
        """Label a patent document with TRIZ parameters."""
        # Load and preprocess document
        text = self.load_patent_document(file_path)
        processed_text = self.preprocess_text(text)
        
        # Extract sections
        sections = self.extract_patent_sections(processed_text)
        
        # Calculate scores for each section
        section_scores = {}
        for section_name, section_text in sections.items():
            if section_text:
                section_scores[section_name] = self.ensemble_scoring(section_text)
        
        # Calculate overall scores (weighted average of sections)
        section_weights = {
            'abstract': 0.3,
            'claims': 0.4,
            'summary': 0.2,
            'description': 0.1
        }
        
        overall_scores = {}
        for param_id in self.triz_parameters.keys():
            weighted_score = 0
            total_weight = 0
            
            for section_name, scores in section_scores.items():
                weight = section_weights.get(section_name, 0.1)
                weighted_score += weight * scores.get(param_id, 0)
                total_weight += weight
            
            overall_scores[param_id] = weighted_score / total_weight if total_weight > 0 else 0
        
        # Filter and sort results
        filtered_scores = {
            param_id: score for param_id, score in overall_scores.items()
            if score >= threshold
        }
        
        # Get top-k parameters
        top_parameters = sorted(
            filtered_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        # Prepare results
        results = {
            'file_path': file_path,
            'top_parameters': [
                {
                    'id': param_id,
                    'name': self.triz_parameters[param_id],
                    'score': score,
                    'confidence': 'high' if score > 0.7 else 'medium' if score > 0.4 else 'low'
                }
                for param_id, score in top_parameters
            ],
            'all_scores': overall_scores,
            'section_analysis': {
                section: {
                    'top_parameter': max(scores.items(), key=lambda x: x[1]) if scores else None,
                    'avg_score': np.mean(list(scores.values())) if scores else 0
                }
                for section, scores in section_scores.items()
            }
        }
        
        return results
    
    def batch_label_patents(self, directory_path: str, output_file: str = None) -> List[Dict]:
        """Label multiple patent documents in a directory."""
        directory = Path(directory_path)
        results = []
        
        # Supported file extensions
        supported_extensions = ['.txt', '.pdf']
        
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in supported_extensions:
                try:
                    print(f"Processing: {file_path.name}")
                    result = self.label_patent(str(file_path))
                    results.append(result)
                except Exception as e:
                    print(f"Error processing {file_path.name}: {e}")
                    continue
        
        # Save results if output file specified
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(results, f, indent=2)
            print(f"Results saved to: {output_file}")
        
        return results
    
    def export_to_csv(self, results: List[Dict], output_file: str):
        """Export results to CSV format."""
        rows = []
        
        for result in results:
            base_row = {
                'file_path': result['file_path'],
                'filename': Path(result['file_path']).name
            }
            
            # Add top parameters
            for i, param in enumerate(result['top_parameters'][:5]):  # Top 5
                base_row[f'param_{i+1}_id'] = param['id']
                base_row[f'param_{i+1}_name'] = param['name']
                base_row[f'param_{i+1}_score'] = param['score']
                base_row[f'param_{i+1}_confidence'] = param['confidence']
            
            rows.append(base_row)
        
        df = pd.DataFrame(rows)
        df.to_csv(output_file, index=False)
        print(f"CSV exported to: {output_file}")


def main():
    """Main function to demonstrate usage."""
    labeler = TRIZPatentLabeler()
    
    # Example usage
    print("TRIZ Patent Labeler")
    print("===================")
    
    # Get user input
    choice = input("Choose option:\n1. Label single patent\n2. Batch label patents\nEnter choice (1 or 2): ")
    
    if choice == '1':
        file_path = input("Enter patent file path: ")
        if os.path.exists(file_path):
            result = labeler.label_patent(file_path)
            
            print(f"\nResults for: {Path(file_path).name}")
            print("-" * 50)
            
            for param in result['top_parameters']:
                print(f"Parameter {param['id']}: {param['name']}")
                print(f"  Score: {param['score']:.3f}")
                print(f"  Confidence: {param['confidence']}")
                print()
        else:
            print("File not found!")
    
    elif choice == '2':
        directory_path = input("Enter directory path containing patents: ")
        output_file = input("Enter output JSON file name (optional): ") or None
        
        if os.path.exists(directory_path):
            results = labeler.batch_label_patents(directory_path, output_file)
            print(f"\nProcessed {len(results)} patents")
            
            # Ask if user wants CSV export
            csv_export = input("Export to CSV? (y/n): ").lower() == 'y'
            if csv_export:
                csv_file = input("Enter CSV filename: ") or "patent_labels.csv"
                labeler.export_to_csv(results, csv_file)
        else:
            print("Directory not found!")
    
    else:
        print("Invalid choice!")


if __name__ == "__main__":
    main()
