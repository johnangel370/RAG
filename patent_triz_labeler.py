import polars as pl
import numpy as np
from typing import List, Dict
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings
import json
import os
from pathlib import Path

class TRIZPatentLabeler:
    def __init__(self):
        """Initialize the TRIZ Patent Labeler with 48 engineering parameters.
        
        REFERENCES:
        - TRIZ Parameters: Altshuller, G. (1984). "Creativity as an Exact Science"
        - Ensemble Methods: Polikar, R. (2006). "Ensemble based systems in decision making"
        - Patent Analysis: Yoon, B. & Park, Y. (2004). "A text-mining-based patent network"
        - See REFERENCES.md for complete bibliography
        
        METHODOLOGY:
        Uses ensemble approach combining:
        1. Keyword matching (30% weight) - Domain-specific term matching
        2. Semantic similarity (40% weight) - Contextual understanding via embeddings  
        3. TF-IDF analysis (30% weight) - Statistical text analysis
        
        LIMITATIONS:
        - Keywords are hand-crafted, not corpus-derived
        - Fixed weights, not learned from training data
        - No validation against ground truth dataset
        """
        self.triz_parameters = {
            1: "Weight of moving object",
            2: "Weight of stationary object", 
            3: "Length or angleof moving object",
            4: "Length or angle of stationary object",
            5: "Area of moving object",
            6: "Area of stationary object",
            7: "Volume of moving object",
            8: "Volume of stationary object",
            9: "Shape",
            10: "Amount of sunstance",
            11: "Amount of information",
            12: "Duration of action of moving object",
            13: "Duration of action of stationary object",
            14: "Speed",
            15: "Force or torque",
            16: "Energy used by moving object",
            17: "Energy used by stationary object",
            18: "Power",
            19: "Stress or pressure",
            20: "Strength",
            21: "Stability",
            22: "Temperature",
            23: "Illumination intensity",
            24: "Function efficiency",
            25: "Loss of substance",
            26: "Loss of time",
            27: "Loss of energy",
            28: "Loss of information",
            29: "Noise",
            30: "Harnful emissions",
            31: "Other harmful effects generated by system",
            32: "Adaptability or versatility",
            33: "Compatibility or connectability",
            34: "Ease of operation",
            35: "Reliability or robustness",
            36: "Repairability",
            37: "Security",
            38: "Safety or vulnerability",
            39: "Aesthetics or appearance",
            40: "Other Harmful Effects Acting On the System",
            41: "Manufacturability",
            42: "Manufacture precision or consistency",
            43: "Automation",
            44: "Productivity",
            45: "System complexity",
            46: "Control complexity",
            47: "Ability to detect or measure",
            48: "Measurement precision"
        }
        
        # Keywords associated with each parameter for better matching
        self.parameter_keywords = {
            1: ["weight", "mass", "heavy", "light", "moving", "mobile", "dynamic"],
            2: ["weight", "mass", "heavy", "light", "stationary", "fixed", "static"],
            3: ["length", "angle", "long", "short", "dimension", "angular", "moving", "mobile"],
            4: ["length", "angle", "long", "short", "dimension", "angular", "stationary", "fixed"],
            5: ["area", "surface", "coverage", "moving", "mobile"],
            6: ["area", "surface", "coverage", "stationary", "fixed"],
            7: ["volume", "capacity", "size", "moving", "mobile"],
            8: ["volume", "capacity", "size", "stationary", "fixed"],
            9: ["shape", "form", "geometry", "configuration", "design", "contour"],
            10: ["amount", "substance", "quantity", "material", "matter", "mass"],
            11: ["amount", "information", "data", "knowledge", "content", "details"],
            12: ["duration", "time", "period", "lifetime", "moving", "action"],
            13: ["duration", "time", "period", "lifetime", "stationary", "action"],
            14: ["speed", "velocity", "fast", "slow", "rate", "acceleration"],
            15: ["force", "torque", "push", "pull", "tension", "compression", "twist"],
            16: ["energy", "power", "consumption", "efficiency", "moving", "dynamic"],
            17: ["energy", "power", "consumption", "efficiency", "stationary", "static"],
            18: ["power", "energy", "electrical", "mechanical", "output", "wattage"],
            19: ["stress", "pressure", "strain", "load", "mechanical", "compression"],
            20: ["strength", "durability", "robust", "strong", "weak", "mechanical"],
            21: ["stability", "stable", "unstable", "equilibrium", "balance", "steady"],
            22: ["temperature", "heat", "cold", "thermal", "heating", "cooling"],
            23: ["illumination", "light", "brightness", "lighting", "visibility"],
            24: ["function", "efficiency", "performance", "effectiveness", "capability"],
            25: ["loss", "waste", "substance", "material", "leakage"],
            26: ["loss", "time", "delay", "efficiency", "speed"],
            27: ["loss", "waste", "energy", "efficiency", "dissipation"],
            28: ["loss", "information", "data", "signal", "communication"],
            29: ["noise", "sound", "vibration", "acoustic", "disturbance"],
            30: ["harmful", "emissions", "pollution", "toxic", "waste", "byproduct"],
            31: ["harmful", "effects", "generated", "system", "internal", "byproduct"],
            32: ["adaptability", "versatility", "flexible", "adjustable", "modular"],
            33: ["compatibility", "connectability", "interoperability", "standard", "interface"],
            34: ["ease", "operation", "use", "user", "interface", "simple"],
            35: ["reliability", "robustness", "dependable", "consistent", "failure"],
            36: ["repairability", "repair", "maintenance", "service", "fix"],
            37: ["security", "secure", "protection", "encryption", "access"],
            38: ["safety", "vulnerability", "hazard", "risk", "danger"],
            39: ["aesthetics", "appearance", "beauty", "visual", "design"],
            40: ["harmful", "effects", "acting", "external", "damage"],
            41: ["manufacturability", "manufacture", "production", "fabrication", "easy"],
            42: ["manufacture", "precision", "consistency", "tolerance", "production", "quality"],
            43: ["automation", "automatic", "manual", "control", "extent"],
            44: ["productivity", "output", "efficiency", "throughput", "performance"],
            45: ["system", "complexity", "simple", "complicated", "device"],
            46: ["control", "complexity", "simple", "complicated", "management"],
            47: ["ability", "detect", "measure", "sensing", "monitoring", "detection"],
            48: ["measurement", "precision", "accuracy", "error", "calibration"]
        }
        
        # Initialize embeddings model
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        
        # Initialize TF-IDF vectorizer
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        
    def load_patent_json(self, json_file_path: str) -> List[Dict]:
        """Load patent documents from JSON file."""
        with open(json_file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def combine_patent_text(self, patent_doc: Dict) -> str:
        """Combine different sections of a patent document into a single text."""
        sections = []
        
        # Add title if available
        if 'title' in patent_doc and patent_doc['title']:
            sections.append(f"Title: {patent_doc['title']}")
        
        # Add abstract if available
        if 'abstract' in patent_doc and patent_doc['abstract']:
            sections.append(f"Abstract: {patent_doc['abstract']}")
        
        # Add description if available
        if 'description' in patent_doc and patent_doc['description']:
            sections.append(f"Description: {patent_doc['description']}")
        
        # Add claims if available
        if 'claims' in patent_doc and patent_doc['claims']:
            sections.append(f"Claims: {patent_doc['claims']}")
        
        return ' '.join(sections)
    
    def preprocess_text(self, text: str) -> str:
        """Preprocess patent text for analysis."""
        # Remove extra whitespace and newlines
        text = re.sub(r'\s+', ' ', text)
        # Convert to lowercase
        text = text.lower()
        # Remove special characters but keep periods and commas
        text = re.sub(r'[^\w\s.,]', '', text)
        return text.strip()
    
    def extract_patent_sections(self, text: str) -> Dict[str, str]:
        """Extract relevant sections from patent text."""
        sections = {
            'abstract': '',
            'claims': '',
            'description': ''
        }
        
        # Simple regex patterns to identify sections
        patterns = {
            'abstract': r'abstract[:\s]*(.*?)(?=\n\n|\nclaims?|background)',
            'claims': r'claims?[:\s]*(.*?)(?=\n\n|description|background)',
            'description': r'(?:detailed\s+)?description[:\s]*(.*?)(?=\n\n|claims?|abstract)'
        }
        
        for section, pattern in patterns.items():
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                sections[section] = match.group(1).strip()
        
        # If no sections found, use entire text as description
        if not any(sections.values()):
            sections['description'] = text
            
        return sections
    
    def calculate_keyword_scores(self, text: str) -> Dict[int, float]:
        """Calculate scores for each TRIZ parameter based on keyword matching."""
        scores = {}
        text_lower = text.lower()
        
        for param_id, keywords in self.parameter_keywords.items():
            score = 0
            for keyword in keywords:
                # Count occurrences of keyword
                count = len(re.findall(r'\b' + re.escape(keyword) + r'\b', text_lower))
                score += count
            
            # Normalize by text length
            scores[param_id] = score / len(text.split()) if text.split() else 0
            
        return scores
    
    def calculate_semantic_similarity(self, text: str) -> Dict[int, float]:
        """Calculate semantic similarity between text and TRIZ parameters."""
        # Create embeddings for the text
        text_embedding = self.embeddings_model.embed_query(text)
        
        scores = {}
        for param_id, param_desc in self.triz_parameters.items():
            # Create embedding for parameter description + keywords
            param_text = param_desc + " " + " ".join(self.parameter_keywords[param_id])
            param_embedding = self.embeddings_model.embed_query(param_text)
            
            # Calculate cosine similarity
            similarity = cosine_similarity(
                [text_embedding], 
                [param_embedding]
            )[0][0]
            scores[param_id] = similarity
            
        return scores
    
    def calculate_tfidf_scores(self, text: str, reference_corpus: List[str] = None) -> Dict[int, float]:
        """Calculate TF-IDF based scores for TRIZ parameters."""
        if reference_corpus is None:
            # Use parameter descriptions as reference corpus
            reference_corpus = [
                f"{desc} {' '.join(self.parameter_keywords[param_id])}"
                for param_id, desc in self.triz_parameters.items()
            ]
        
        # Add the input text to corpus
        corpus = reference_corpus + [text]
        # print(corpus)
        
        # Fit TF-IDF vectorizer
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(corpus)
        
        # Calculate similarity between text and each parameter
        text_vector = tfidf_matrix[-1]  # Last item is the input text
        param_vectors = tfidf_matrix[:-1]  # All others are parameter descriptions
        
        similarities = cosine_similarity(text_vector, param_vectors)[0]
        
        scores = {}
        for i, param_id in enumerate(self.triz_parameters.keys()):
            scores[param_id] = similarities[i]
            
        return scores
    
    def ensemble_scoring(self, text: str, weights: Dict[str, float] = None) -> Dict[int, float]:
        """Combine multiple scoring methods using ensemble approach."""
        if weights is None:
            weights = {
                'keyword': 0.2,
                'semantic': 0.5,
                'tfidf': 0.3
            }
        
        # Calculate scores using different methods
        keyword_scores = self.calculate_keyword_scores(text)
        semantic_scores = self.calculate_semantic_similarity(text)
        tfidf_scores = self.calculate_tfidf_scores(text)
        
        # Normalize scores to 0-1 range
        def normalize_scores(scores):
            max_score = max(scores.values()) if scores.values() else 1
            if max_score == 0.0:
                max_score = 1
            return {k: v / max_score for k, v in scores.items()}
        
        keyword_scores = normalize_scores(keyword_scores)
        # print("keyword_scores: ", keyword_scores)
        semantic_scores = normalize_scores(semantic_scores)
        # print("semantic_scores: ", semantic_scores)
        tfidf_scores = normalize_scores(tfidf_scores)
        # print("tfidf_scores: ", tfidf_scores)
        
        # Combine scores
        final_scores = {}
        for param_id in self.triz_parameters.keys():
            final_scores[param_id] = (
                weights['keyword'] * keyword_scores.get(param_id, 0) +
                weights['semantic'] * semantic_scores.get(param_id, 0) +
                weights['tfidf'] * tfidf_scores.get(param_id, 0)
            )
        
        return final_scores
    
    def label_patent_from_json(self, patent_doc: Dict, top_k: int = 5, threshold: float = 0.1) -> Dict:
        """Label a patent document from JSON data with TRIZ parameters."""
        # Combine patent text from JSON fields
        text = self.combine_patent_text(patent_doc)
        processed_text = self.preprocess_text(text)
        
        # Extract sections from the combined text
        sections = self.extract_patent_sections(processed_text)
        
        # Calculate scores for each section
        section_scores = {}
        for section_name, section_text in sections.items():
            if section_text:
                section_scores[section_name] = self.ensemble_scoring(section_text)
        
        # Calculate overall scores (weighted average of sections)
        section_weights = {
            'abstract': 0.2,
            'claims': 0.3,
            'description': 0.5
        }
        
        overall_scores = {}
        for param_id in self.triz_parameters.keys():
            weighted_score = 0
            total_weight = 0
            
            for section_name, scores in section_scores.items():
                if section_name in section_weights:
                    weight = section_weights[section_name]
                    weighted_score += weight * scores.get(param_id, 0)
                    total_weight += weight
            
            overall_scores[param_id] = weighted_score / total_weight if total_weight > 0 else 0
        
        # Filter and sort results
        filtered_scores = {
            param_id: score for param_id, score in overall_scores.items()
            if score >= threshold
        }
        
        # Get top-k parameters
        top_parameters = sorted(
            filtered_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        # Prepare results
        results = {
            'patent_id': patent_doc.get('id', 'unknown'),
            'doc_num': patent_doc.get('doc_num', 'unknown'),
            'title': patent_doc.get('title', 'unknown'),
            'top_parameters': [
                {
                    'id': param_id,
                    'name': self.triz_parameters[param_id],
                    'score': score,
                    'confidence': 'high' if score > 0.7 else 'medium' if score > 0.4 else 'low'
                }
                for param_id, score in top_parameters
            ],
            'all_scores': overall_scores,
            'section_analysis': {
                section: {
                    'top_parameter': max(scores.items(), key=lambda x: x[1]) if scores else None,
                    'avg_score': np.mean(list(scores.values())) if scores else 0
                }
                for section, scores in section_scores.items()
            }
        }
        
        return results
    
    def batch_label_patents_from_json(self, json_file_path: str, output_file: str = None, limit: int = None) -> List[Dict]:
        """Label multiple patent documents from JSON file."""
        print(f"Loading patents from: {json_file_path}")
        patent_docs = self.load_patent_json(json_file_path)
        
        if limit:
            patent_docs = patent_docs[:limit]
            print(f"Processing first {limit} patents...")
        
        results = []
        total_patents = len(patent_docs)
        
        for i, patent_doc in enumerate(patent_docs, 1):
            try:
                print(f"Processing patent {i}/{total_patents}: {patent_doc.get('title', 'Unknown title')[:50]}...")
                result = self.label_patent_from_json(patent_doc)
                results.append(result)
            except Exception as e:
                print(f"Error processing patent {patent_doc.get('id', 'unknown')}: {e}")
                continue
        
        # Save results if output file specified
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"Results saved to: {output_file}")
        
        return results
    
    def export_to_csv(self, results: List[Dict], output_file: str):
        """Export results to CSV format using polars."""
        rows = []
        
        for result in results:
            # JSON-based result
            base_row = {
                'patent_id': result.get('patent_id', 'unknown'),
                'doc_num': result.get('doc_num', 'unknown'),
                'title': result.get('title', 'unknown')[:100] + '...' if len(result.get('title', '')) > 100 else result.get('title', 'unknown')
            }
            
            # Add top parameters
            for i, param in enumerate(result['top_parameters'][:5]):  # Top 5
                base_row[f'param_{i+1}_id'] = param['id']
                base_row[f'param_{i+1}_name'] = param['name']
                base_row[f'param_{i+1}_score'] = param['score']
                base_row[f'param_{i+1}_confidence'] = param['confidence']
            
            rows.append(base_row)
        
        # Create polars DataFrame and export to CSV
        df = pl.DataFrame(rows)
        df.write_csv(output_file)
        print(f"CSV exported to: {output_file}")


def main():
    """Main function to demonstrate usage."""
    labeler = TRIZPatentLabeler()
    
    print("TRIZ Patent Labeler - JSON Processing")
    print("=====================================")
    
    json_file_path = input("Enter JSON file path: ")
    if os.path.exists(json_file_path):
        limit_input = input("Enter limit (number of patents to process, or press Enter for all): ")
        limit = int(limit_input) if limit_input.strip() else None
        
        output_file = input("Enter output JSON file name (or press Enter for default): ") or "patent_triz_results.json"
        
        results = labeler.batch_label_patents_from_json(json_file_path, output_file, limit)
        print(f"\nProcessed {len(results)} patents")
        print(f"Results saved to: {output_file}")
        
        # Ask if user wants CSV export
        csv_export = input("\nExport to CSV? (y/n): ").lower() == 'y'
        if csv_export:
            csv_file = input("Enter CSV filename (or press Enter for default): ") or "patent_triz_results.csv"
            labeler.export_to_csv(results, csv_file)
    else:
        print("JSON file not found!")


if __name__ == "__main__":
    main()
